import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.flink.api.connector.sink2.SinkWriter;
import com.example.avro.User;

import java.util.Map;

public class AvroConfluentSerializationSchema implements KafkaRecordSerializationSchema<User> {

    private final String topic;
    private transient KafkaAvroSerializer serializer;

    public AvroConfluentSerializationSchema(String topic) {
        this.topic = topic;
    }

    @Override
    public void open(InitializationContext context, SinkWriter.Context sinkContext) {
        serializer = new KafkaAvroSerializer();
        serializer.configure(
            Map.of(
                "schema.registry.url", "http://localhost:8081",
                "value.subject.name.strategy", "io.confluent.kafka.serializers.subject.TopicNameStrategy",
                "auto.register.schemas", "true"
            ),
            false // isKeySerializer = false
        );
    }

    @Override
    public ProducerRecord<byte[], byte[]> serialize(User element, KafkaSinkContext context) {
        byte[] valueBytes = serializer.serialize(topic, element);
        return new ProducerRecord<>(topic, null, valueBytes);
    }
}


StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream<User> stream = env.fromElements(
    User.newBuilder().setName("Alice").setAge(30).build(),
    User.newBuilder().setName("Bob").setAge(42).build()
);

stream.sinkTo(sink);

env.execute("Flink Confluent Avro Producer");


KafkaSink<User> sink = KafkaSink.<User>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(new AvroConfluentSerializationSchema("users-topic"))
    .build();
